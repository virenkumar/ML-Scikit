{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7abe85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce2280cb",
   "metadata": {},
   "source": [
    "Q1. What Linear Regression training algorithm can you use if you have a training set\n",
    "with millions of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389335c",
   "metadata": {},
   "source": [
    "If the number of features is high, it's better to avoid solving linear regression using Normal equations because matrix operations with so much of data is extremely computing intensive.\n",
    "We can use gradient descent to solve linear regression. If the number of training instances (m) is not very high, we can use Batch gradient descent. Else, we can use Stochastic gradient descent or Mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf0a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "482ff595",
   "metadata": {},
   "source": [
    "Q2. Suppose the features in your training set have very different scales. What algo‐\n",
    "rithms might suffer from this, and how? What can you do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2764fe3",
   "metadata": {},
   "source": [
    "If the features in the training set have very different scales, algorithms like gradient descent suffer. The cost function will be elongated bowl shape if the scales are quite different. The descent path will then be a long march over almost flat trajectory. In essence, it will take much longer to converge to the minima.\n",
    "We can normalize the features to bring them to similar scale to overcome this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361c095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb5d1e9",
   "metadata": {},
   "source": [
    "Q3. Can Gradient Descent get stuck in a local minimum when training a Logistic\n",
    "Regression model?\n",
    "\n",
    "The log loss function for Logistic regression model is convex. So, for this loss function there will be only one minima, which will be the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b82a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c372d72",
   "metadata": {},
   "source": [
    "Q4. Do all Gradient Descent algorithms lead to the same model provided you let\n",
    "them run long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83367dd",
   "metadata": {},
   "source": [
    "Batch gradient descent converges to the minima. Stochastic gradient descent does not converge to the minima but it will keep moving around it when it reaches close enough. So, it's not necessary that it will lead to the same model. This (the convergence) is assuming that we're not using very high learning rate which might take us away from the minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be83cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba131ca4",
   "metadata": {},
   "source": [
    "Q5. Suppose you use Batch Gradient Descent and you plot the validation error at\n",
    "every epoch. If you notice that the validation error consistently goes up, what is\n",
    "likely going on? How can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b0ae4",
   "metadata": {},
   "source": [
    "If the validation error goes up with every epoch in Batch gradient descent, then we are moving further away from the minima. We can try with a lower learning rate to see the effect on validation error.\n",
    "However, if the training error is not going up, it means that the model has high variance. We don't need to train the model any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066d7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ff8c2b8",
   "metadata": {},
   "source": [
    "Q6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\n",
    "dation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302d492",
   "metadata": {},
   "source": [
    "No, it would be better to see for a few iterations that the validation error remains above the minimum. Mini batch gradient descent may not move consistently in the right direction in every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2990bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6518393a",
   "metadata": {},
   "source": [
    "Q7. Which Gradient Descent algorithm (among those we discussed) will reach the\n",
    "vicinity of the optimal solution the fastest? Which will actually converge? How\n",
    "can you make the others converge as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa42e4",
   "metadata": {},
   "source": [
    "Stochastic gradient descent will reach the vicinity of the optimal solution the fastest. Batch gradient descent will converge to the minimum. To make stochastic gradient descent or mini batch gradient descent converge, we can use variable learning rates - higher initailly and then moving towards zero. We can also stop early when we see that the validation error is increasing for few iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb1ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "627fef54",
   "metadata": {},
   "source": [
    "Q8. Suppose you are using Polynomial Regression. You plot the learning curves and\n",
    "you notice that there is a large gap between the training error and the validation\n",
    "error. What is happening? What are three ways to solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48a955",
   "metadata": {},
   "source": [
    "If there's a large gap between training error and validation error, it means the model is performing much better for instances it has seen as compared to new instances. We can handle this by\n",
    "- training the model on more data \n",
    "- use regularization so that the model does not overfit the training data\n",
    "- try to use a simpler model, such as polynomial of lower degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd6817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b95c53d2",
   "metadata": {},
   "source": [
    "Q9. Suppose you are using Ridge Regression and you notice that the training error\n",
    "and the validation error are almost equal and fairly high. Would you say that the\n",
    "model suffers from high bias or high variance? Should you increase the regulari‐\n",
    "zation hyperparameter α or reduce it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ae02c",
   "metadata": {},
   "source": [
    "If both the training error and validation error are high, the model suffers from high bias. We should reduce the value of regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a0159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c77b942",
   "metadata": {},
   "source": [
    "Q10. Why would you want to use:\n",
    "• Ridge Regression instead of Linear Regression?\n",
    "• Lasso instead of Ridge Regression?\n",
    "• Elastic Net instead of Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41dce04",
   "metadata": {},
   "source": [
    "- Ridge regression uses L2 regularization for solving linear regression. It's almost always better to use some regularization. So, it would make sense to use Ridge regression as the default.\n",
    "- Lasso uses L1 regularization. Lasso can be used when there are large number of features and only some of them are informative, as it does feature selection by itself.\n",
    "- In most of the cases, it's better to use Elastic Net instead of Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a36f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc141b7e",
   "metadata": {},
   "source": [
    "Q11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\n",
    "Should you implement two Logistic Regression classifiers or one Softmax Regres‐\n",
    "sion classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3798ab4",
   "metadata": {},
   "source": [
    "The possibilities in this case are outdoor (daytime or nighttime) or indoor(daytime or nighttime). We cannot use one Softmax regression classifier for this. We'll need 2 logistic regression classifiers for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adf1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "333422f4",
   "metadata": {},
   "source": [
    "Q12. Implement Batch Gradient Descent with early stopping for Softmax Regression\n",
    "(without using Scikit-Learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c2117f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
